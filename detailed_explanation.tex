----------------------------------------------------------------------------------
HISTOGRAM📊
----------------------------------------------------------------------------------
A histogram is a graphical representation of the distribution of numerical data. It is similar to a bar chart but represents frequency distributions.

Key Points about Histograms:
➤It consists of bars where each bar represents a range (bin) of values.
➤The height of the bars indicates the frequency (count) of data points in each range.
➤It helps to visualize the shape of data distributions (e.g., normal, skewed, uniform).
➤Unlike bar charts, histograms are for continuous data, not categorical data.

🛠 Example:
If you collect ages of 100 people, a histogram can show how many people fall into different age groups (e.g., 0-10, 11-20, 21-30, etc.).

------------------------------------------------------------------------------------
BOXPLOT📦
------------------------------------------------------------------------------------
A boxplot (also called a box-and-whisker plot) is a statistical chart that shows the distribution of a dataset using five key summary values:

Boxplot Components:
🔸Minimum (smallest value, excluding outliers)
🔸First Quartile (Q1) (25th percentile)
🔸Median (Q2) (50th percentile, middle value)
🔸Third Quartile (Q3) (75th percentile)
🔸Maximum (largest value, excluding outliers)
🔸Outliers (shown as individual points beyond the whiskers)

🔍 Why Use a Boxplot?
Helps identify outliers 📌
Shows spread & skewness of data
Useful for comparing multiple datasets

----------------------------------------------------------------------------------
LINEAR REGRESSION📈
----------------------------------------------------------------------------------
Linear regression is a supervised learning algorithm used to model the relationship between a dependent variable (Y) and one or more independent variables (X) using a straight line.

🔢 Formula (Simple Linear Regression):

Y=mX+c
Where:
Y = Predicted value (dependent variable)
m = Slope of the line (coefficient)
X = Independent variable
c = Intercept (where the line crosses the Y-axis)

🛠 Types of Linear Regression:
Simple Linear Regression – One independent variable (e.g., predicting house price based on size).
Multiple Linear Regression – Multiple independent variables (e.g., predicting sales based on ad spend, price, and season).

📊 Why Use Linear Regression?
✅ Easy to interpret and implement
✅ Good for predicting trends
✅ Helps understand relationships between variables

----------------------------------------------------------------------------
DECISION TREE🌳
----------------------------------------------------------------------------
A Decision Tree is a machine learning algorithm that splits data into branches based on decision rules. It is used for both classification and regression tasks.

🔢 How It Works?
Starts with a root node (entire dataset).
Splits data based on a feature (e.g., "Is temperature > 30°C?").
Continues splitting into branches and leaves until:
A stopping condition is met (e.g., max depth reached).
The leaf node contains homogeneous data (pure class).

📊 Key Terminologies:
Root Node 🌱 → The starting point (entire dataset).
Decision Node 📍 → A node that splits based on a condition.
Leaf Node 🍃 → The final outcome (prediction).
Depth 📏 → The number of splits from root to leaf.
Gini Impurity / Entropy 📊 → Measures how pure the nodes are.

✅ Advantages:
✔️ Easy to understand & visualize
✔️ Works well with small datasets
✔️ Handles both numerical & categorical data

❌ Disadvantages:
❌ Can overfit 
❌ Sensitive to small changes in data

----------------------------------------------------------------------------
RANDOM FOREST REGRESSOR🌲
----------------------------------------------------------------------------
Random Forest Regressor is an ensemble learning method that uses multiple decision trees to make predictions. It improves accuracy and reduces overfitting compared to a single decision tree.

🔢 How It Works?
Bootstrapping: Multiple random subsets of the dataset are created.
Decision Trees: Each subset is used to train a different decision tree.
Averaging: Predictions from all trees are averaged to get the final output.

📊 Why Use Random Forest Regression?
✅ Handles non-linear relationships 📈
✅ Reduces overfitting (compared to a single tree)
✅ Works well with large datasets
✅ Can handle missing values
