----------------------------------------------------------------------------------
HISTOGRAMğŸ“Š
----------------------------------------------------------------------------------
A histogram is a graphical representation of the distribution of numerical data. It is similar to a bar chart but represents frequency distributions.

Key Points about Histograms:
â¤It consists of bars where each bar represents a range (bin) of values.
â¤The height of the bars indicates the frequency (count) of data points in each range.
â¤It helps to visualize the shape of data distributions (e.g., normal, skewed, uniform).
â¤Unlike bar charts, histograms are for continuous data, not categorical data.

ğŸ›  Example:
If you collect ages of 100 people, a histogram can show how many people fall into different age groups (e.g., 0-10, 11-20, 21-30, etc.).

------------------------------------------------------------------------------------
BOXPLOTğŸ“¦
------------------------------------------------------------------------------------
A boxplot (also called a box-and-whisker plot) is a statistical chart that shows the distribution of a dataset using five key summary values:

Boxplot Components:
ğŸ”¸Minimum (smallest value, excluding outliers)
ğŸ”¸First Quartile (Q1) (25th percentile)
ğŸ”¸Median (Q2) (50th percentile, middle value)
ğŸ”¸Third Quartile (Q3) (75th percentile)
ğŸ”¸Maximum (largest value, excluding outliers)
ğŸ”¸Outliers (shown as individual points beyond the whiskers)

ğŸ” Why Use a Boxplot?
Helps identify outliers ğŸ“Œ
Shows spread & skewness of data
Useful for comparing multiple datasets

----------------------------------------------------------------------------------
LINEAR REGRESSIONğŸ“ˆ
----------------------------------------------------------------------------------
Linear regression is a supervised learning algorithm used to model the relationship between a dependent variable (Y) and one or more independent variables (X) using a straight line.

ğŸ”¢ Formula (Simple Linear Regression):

Y=mX+c
Where:
Y = Predicted value (dependent variable)
m = Slope of the line (coefficient)
X = Independent variable
c = Intercept (where the line crosses the Y-axis)

ğŸ›  Types of Linear Regression:
Simple Linear Regression â€“ One independent variable (e.g., predicting house price based on size).
Multiple Linear Regression â€“ Multiple independent variables (e.g., predicting sales based on ad spend, price, and season).

ğŸ“Š Why Use Linear Regression?
âœ… Easy to interpret and implement
âœ… Good for predicting trends
âœ… Helps understand relationships between variables

----------------------------------------------------------------------------
DECISION TREEğŸŒ³
----------------------------------------------------------------------------
A Decision Tree is a machine learning algorithm that splits data into branches based on decision rules. It is used for both classification and regression tasks.

ğŸ”¢ How It Works?
Starts with a root node (entire dataset).
Splits data based on a feature (e.g., "Is temperature > 30Â°C?").
Continues splitting into branches and leaves until:
A stopping condition is met (e.g., max depth reached).
The leaf node contains homogeneous data (pure class).

ğŸ“Š Key Terminologies:
Root Node ğŸŒ± â†’ The starting point (entire dataset).
Decision Node ğŸ“ â†’ A node that splits based on a condition.
Leaf Node ğŸƒ â†’ The final outcome (prediction).
Depth ğŸ“ â†’ The number of splits from root to leaf.
Gini Impurity / Entropy ğŸ“Š â†’ Measures how pure the nodes are.

âœ… Advantages:
âœ”ï¸ Easy to understand & visualize
âœ”ï¸ Works well with small datasets
âœ”ï¸ Handles both numerical & categorical data

âŒ Disadvantages:
âŒ Can overfit 
âŒ Sensitive to small changes in data

----------------------------------------------------------------------------
RANDOM FOREST REGRESSORğŸŒ²
----------------------------------------------------------------------------
Random Forest Regressor is an ensemble learning method that uses multiple decision trees to make predictions. It improves accuracy and reduces overfitting compared to a single decision tree.

ğŸ”¢ How It Works?
Bootstrapping: Multiple random subsets of the dataset are created.
Decision Trees: Each subset is used to train a different decision tree.
Averaging: Predictions from all trees are averaged to get the final output.

ğŸ“Š Why Use Random Forest Regression?
âœ… Handles non-linear relationships ğŸ“ˆ
âœ… Reduces overfitting (compared to a single tree)
âœ… Works well with large datasets
âœ… Can handle missing values
